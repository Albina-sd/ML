import pandas as pd
import numpy as np

from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import AdaBoostClassifier

from sklearn.model_selection import KFold
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

data = pd.read_csv("all_templates.csv", header=None)
target = data[data.shape[1] - 1]
data = data.drop([data.shape[1] - 1], axis=1)
% matplotlib
inline
import matplotlib.pyplot as plt

plt.rcParams["figure.figsize"] = (7, 7)  # (w, h)
plt.rcParams.update({'font.size': 10})

classifiers = {
    'Gaussian Naive Bayes': GaussianNB(),
    'Multinomial Naive Bayes': MultinomialNB(),
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(n_estimators=30, n_jobs=-1),
    'Decision Tree': DecisionTreeClassifier(random_state=241),
    'Ada Boost': AdaBoostClassifier()
}

k_split = 4
kf = KFold(n_splits=k_split, shuffle=True, random_state=42)

X = np.array(data)
y = np.array(target)
res = np.zeros((len(classifiers), k_split))
tprs = []
aucs = []
original = []
predicted = []

for i in range(len(classifiers)):
    tprs.append([])
    aucs.append([])
    original.append(np.array([]))
    predicted.append(np.array([]))

mean_fpr = np.linspace(0, 1, 100)

for train_index, test_index in kf.split(y):

    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    for idx, (name, classifier) in enumerate(classifiers.items()):
        print("Определение показателей для {}".format(name))
        model = classifier.fit(X_train, y_train)
        pred = model.predict(X_test)
        probas_ = model.predict_proba(X_test)
        fpr, tpr, thresholds = roc_curve(y_test,probas_[:,probas_.shape[1]- 1])
        tprs[idx].append(np.interp(mean_fpr, fpr, tpr))
        tprs[idx][-1][0] = 0.0
        aucs[idx].append(auc(fpr, tpr))
        original[idx] = np.hstack([original[idx], y_test])
        predicted[idx] = np.hstack([predicted[idx], pred])
        res[idx] = cross_val_score(model, X, y, cv=k_split, scoring="accuracy")

for idx, (name, classifier) in enumerate(classifiers.items()):
    print(format(name), np.mean(res[i]))

def print_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    print('True positive = ', cm[0][0])
    print('False positive = ', cm[0][1])
    print('False negative = ', cm[1][0])
    print('True negative = ', cm[1][1])


for idx, (name, classifier) in enumerate(classifiers.items()):
    report = classification_report(original[idx], predicted[idx])
    print(name)
    print(report)
    print_confusion_matrix(original[idx], predicted[idx])

plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=.1)
for idx, (name, classifier) in enumerate(classifiers.items()):
    mean_tpr = np.mean(tprs[idx], axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
    std_auc = np.std(aucs[idx])
    plt.plot(mean_fpr, mean_tpr, label=r'%s (ROC AUC = %0.2f $\pm$ %0.2f)' % (name, mean_auc, std_auc), lw=2, alpha=.8)

plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc="lower right")
plt.title("ROC AUC")
plt.show()
