{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = (np.exp(Z)-np.exp(-Z))/(np.exp(-Z)+np.exp(Z))\n",
    "    #A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    s = (np.exp(Z)-np.exp(-Z))/(np.exp(-Z)+np.exp(Z))\n",
    "    dZ = dA * (1 - s**2)\n",
    "    \n",
    "    #s = 1/(1+np.exp(-Z))\n",
    "    #dZ = dA * s * (1-s)\n",
    "    \n",
    "    #assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,A)+b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector \n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(len(Y)):\n",
    "        cost += (Y[i] - AL[i])**2\n",
    "    #cost = -np.sum(np.multiply(Y, np.log(AL)) + np.multiply(np.ones(Y.shape) - Y, np.log(1 - AL))) / m\n",
    "    cost = np.sqrt(cost)\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ,A_prev.T)/m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "        \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db =  linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation = \"sigmoid\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(1, L + 1, 1):\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W%s' % l], parameters['b%s' % l], 'sigmoid')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W%s' % str(l + 1)], parameters['b%s' % str(l + 1)], 'sigmoid')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [-3]\n",
    "train_x = np.array(train_x).T\n",
    "train_x = train_x.reshape(1,1)\n",
    "train_y = [0.3,0.1,0.1]\n",
    "train_y = np.array(train_y)\n",
    "train_y = train_y.reshape(train_y.shape[0],1)\n",
    "#train_y = train_y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x = 1\n",
    "n_h = 1\n",
    "n_y = 3\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 1, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[0]                          # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    eps = 10\n",
    "    i = 0\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "    #while (eps > 0.001):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, activation=\"sigmoid\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, activation=\"sigmoid\")\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(A2, Y)\n",
    "        print('y = ',A2)\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation=\"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation=\"sigmoid\")\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "        print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "        print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "        print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost:\n",
    "            costs.append(cost)\n",
    "            \n",
    "        eps = costs[i-1] - costs[i]\n",
    "        i += 1\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y =  [[0.49925393]\n",
      " [0.49935587]\n",
      " [0.49869146]]\n",
      "W1 = [[0.01054229]]\n",
      "b1 = [[0.00190039]]\n",
      "W2 = [[-0.10331758]\n",
      " [-0.20009543]\n",
      " [-0.20521928]]\n",
      "b2 = [[-0.19925393]\n",
      " [-0.39935587]\n",
      " [-0.39869146]]\n",
      "Cost after iteration 0: 0.5984497671150254\n",
      "y =  [[0.43778782]\n",
      " [0.37803039]\n",
      " [0.37759329]]\n",
      "W1 = [[-0.08456371]]\n",
      "b1 = [[0.03360239]]\n",
      "W2 = [[-0.17118758]\n",
      " [-0.33704456]\n",
      " [-0.34195311]]\n",
      "b2 = [[-0.33704175]\n",
      " [-0.67738626]\n",
      " [-0.67628475]]\n",
      "Cost after iteration 1: 0.4163465037971298\n",
      "y =  [[0.39296952]\n",
      " [0.29526448]\n",
      " [0.29491026]]\n",
      "W1 = [[-0.19358233]]\n",
      "b1 = [[0.06994193]]\n",
      "W2 = [[-0.22430418]\n",
      " [-0.44860568]\n",
      " [-0.45331185]]\n",
      "b2 = [[-0.43001128]\n",
      " [-0.87265074]\n",
      " [-0.87119501]]\n",
      "Cost after iteration 2: 0.29113838559264893\n",
      "y =  [[0.35952732]\n",
      " [0.23731459]\n",
      " [0.23701842]]\n",
      "W1 = [[-0.28622373]]\n",
      "b1 = [[0.10082239]]\n",
      "W2 = [[-0.26342349]\n",
      " [-0.53884412]\n",
      " [-0.54335566]]\n",
      "b2 = [[-0.4895386 ]\n",
      " [-1.00996533]\n",
      " [-1.00821343]]\n",
      "Cost after iteration 3: 0.20291093398954857\n",
      "y =  [[0.33626149]\n",
      " [0.19788584]\n",
      " [0.19764627]]\n",
      "W1 = [[-0.35552676]]\n",
      "b1 = [[0.1239234]]\n",
      "W2 = [[-0.28964129]\n",
      " [-0.60961758]\n",
      " [-0.61395591]]\n",
      "b2 = [[-0.52580009]\n",
      " [-1.10785117]\n",
      " [-1.1058597 ]]\n",
      "Cost after iteration 4: 0.14293819662430818\n",
      "y =  [[0.32127773]\n",
      " [0.17145861]\n",
      " [0.171269  ]]\n",
      "W1 = [[-0.40567048]]\n",
      "b1 = [[0.14063798]]\n",
      "W2 = [[-0.30595772]\n",
      " [-0.66441426]\n",
      " [-0.66860719]]\n",
      "b2 = [[-0.54707782]\n",
      " [-1.17930978]\n",
      " [-1.1771287 ]]\n",
      "Cost after iteration 5: 0.10314235194910491\n",
      "y =  [[0.31207642]\n",
      " [0.1534532 ]\n",
      " [0.15330336]]\n",
      "W1 = [[-0.44221597]]\n",
      "b1 = [[0.15281981]]\n",
      "W2 = [[-0.31556303]\n",
      " [-0.70692973]\n",
      " [-0.71100348]]\n",
      "b2 = [[-0.55915424]\n",
      " [-1.23276298]\n",
      " [-1.23043206]]\n",
      "Cost after iteration 6: 0.0764482349303936\n",
      "y =  [[0.3065756 ]\n",
      " [0.14081326]\n",
      " [0.14069387]]\n",
      "W1 = [[-0.46934983]]\n",
      "b1 = [[0.16186443]]\n",
      "W2 = [[-0.3209188 ]\n",
      " [-0.74017181]\n",
      " [-0.74414832]]\n",
      "b2 = [[-0.56572984]\n",
      " [-1.27357623]\n",
      " [-1.27112593]]\n",
      "Cost after iteration 7: 0.0580082022081537\n",
      "y =  [[0.30335394]\n",
      " [0.13167043]\n",
      " [0.13157427]]\n",
      "W1 = [[-0.48988525]]\n",
      "b1 = [[0.16870957]]\n",
      "W2 = [[-0.3236951 ]\n",
      " [-0.76638768]\n",
      " [-0.7702846 ]]\n",
      "b2 = [[-0.56908378]\n",
      " [-1.30524667]\n",
      " [-1.3027002 ]]\n",
      "Cost after iteration 8: 0.044846402897785416\n",
      "y =  [[0.30151   ]\n",
      " [0.12488603]\n",
      " [0.12480775]]\n",
      "W1 = [[-0.50568823]]\n",
      "b1 = [[0.17397723]]\n",
      "W2 = [[-0.32495944]\n",
      " [-0.78722506]\n",
      " [-0.79105643]]\n",
      "b2 = [[-0.57059378]\n",
      " [-1.3301327 ]\n",
      " [-1.32750795]]\n",
      "Cost after iteration 9: 0.03517128084624925\n",
      "y =  [[0.30048763]\n",
      " [0.11974629]\n",
      " [0.11968197]]\n",
      "W1 = [[-0.51801741]]\n",
      "b1 = [[0.17808695]]\n",
      "W2 = [[-0.32537117]\n",
      " [-0.80389806]\n",
      " [-0.80767512]]\n",
      "b2 = [[-0.57108141]\n",
      " [-1.34987899]\n",
      " [-1.34718992]]\n",
      "Cost after iteration 10: 0.027884289452693344\n",
      "y =  [[0.29994817]\n",
      " [0.1157876 ]\n",
      " [0.11573435]]\n",
      "W1 = [[-0.52774315]]\n",
      "b1 = [[0.18132887]]\n",
      "W2 = [[-0.32532713]\n",
      " [-0.81731257]\n",
      " [-0.82104438]]\n",
      "b2 = [[-0.57102958]\n",
      " [-1.36566659]\n",
      " [-1.36292427]]\n",
      "Cost after iteration 11: 0.02228948027307662\n",
      "y =  [[0.29968732]\n",
      " [0.11269823]\n",
      " [0.11265386]]\n",
      "W1 = [[-0.535483]]\n",
      "b1 = [[0.18390882]]\n",
      "W2 = [[-0.32506017]\n",
      " [-0.82815406]\n",
      " [-0.83184798]]\n",
      "b2 = [[-0.5707169 ]\n",
      " [-1.37836482]\n",
      " [-1.37557813]]\n",
      "Cost after iteration 12: 0.017929387015300103\n",
      "y =  [[0.29958306]\n",
      " [0.11026188]\n",
      " [0.11022471]]\n",
      "W1 = [[-0.54168572]]\n",
      "b1 = [[0.18597639]]\n",
      "W2 = [[-0.32470286]\n",
      " [-0.83694819]\n",
      " [-0.84061027]]\n",
      "b2 = [[-0.57029996]\n",
      " [-1.3886267 ]\n",
      " [-1.38580284]]\n",
      "Cost after iteration 13: 0.014492226749301292\n",
      "y =  [[0.2995634 ]\n",
      " [0.10832434]\n",
      " [0.10829308]]\n",
      "W1 = [[-0.54668431]]\n",
      "b1 = [[0.18764259]]\n",
      "W2 = [[-0.32432761]\n",
      " [-0.84410285]\n",
      " [-0.84773806]]\n",
      "b2 = [[-0.56986336]\n",
      " [-1.39695104]\n",
      " [-1.39409592]]\n",
      "Cost after iteration 14: 0.011758423014747856\n",
      "y =  [[0.29958657]\n",
      " [0.1067731 ]\n",
      " [0.10674672]]\n",
      "W1 = [[-0.55073042]]\n",
      "b1 = [[0.18899129]]\n",
      "W2 = [[-0.32397145]\n",
      " [-0.84993778]\n",
      " [-0.85355027]]\n",
      "b2 = [[-0.56944992]\n",
      " [-1.40372414]\n",
      " [-1.40084265]]\n",
      "Cost after iteration 15: 0.009568909892365165\n",
      "y =  [[0.29962883]\n",
      " [0.10552438]\n",
      " [0.10550206]]\n",
      "W1 = [[-0.55401714]]\n",
      "b1 = [[0.19008686]]\n",
      "W2 = [[-0.32365109]\n",
      " [-0.85470581]\n",
      " [-0.85829904]]\n",
      "b2 = [[-0.56907876]\n",
      " [-1.40924851]\n",
      " [-1.40634471]]\n",
      "Cost after iteration 16: 0.007805716655633671\n",
      "y =  [[0.29967717]\n",
      " [0.10451476]\n",
      " [0.10449585]]\n",
      "W1 = [[-0.55669456]]\n",
      "b1 = [[0.19097934]]\n",
      "W2 = [[-0.32337205]\n",
      " [-0.85860827]\n",
      " [-0.86218516]]\n",
      "b2 = [[-0.56875592]\n",
      " [-1.41376328]\n",
      " [-1.41084056]]\n",
      "Cost after iteration 17: 0.006379654775409484\n",
      "y =  [[0.29972477]\n",
      " [0.10369557]\n",
      " [0.10367951]]\n",
      "W1 = [[-0.55888058]]\n",
      "b1 = [[0.19170801]]\n",
      "W2 = [[-0.32313386]\n",
      " [-0.8618065 ]\n",
      " [-0.86536948]]\n",
      "b2 = [[-0.56848069]\n",
      " [-1.41745885]\n",
      " [-1.41452007]]\n",
      "Cost after iteration 18: 0.0052222384593601555\n",
      "y =  [[0.2997684 ]\n",
      " [0.10302895]\n",
      " [0.1030153 ]]\n",
      "W1 = [[-0.56066866]]\n",
      "b1 = [[0.19230404]]\n",
      "W2 = [[-0.32293323]\n",
      " [-0.86443038]\n",
      " [-0.86798154]]\n",
      "b2 = [[-0.56824909]\n",
      " [-1.4204878 ]\n",
      " [-1.41753537]]\n",
      "Cost after iteration 19: 0.0042802123169592724\n",
      "y =  [[0.29980679]\n",
      " [0.10248521]\n",
      " [0.1024736 ]]\n",
      "W1 = [[-0.56213338]]\n",
      "b1 = [[0.19279228]]\n",
      "W2 = [[-0.32276573]\n",
      " [-0.86658495]\n",
      " [-0.87012604]]\n",
      "b2 = [[-0.56805589]\n",
      " [-1.42297301]\n",
      " [-1.42000897]]\n",
      "Cost after iteration 20: 0.0035117381809345056\n",
      "y =  [[0.29983975]\n",
      " [0.10204085]\n",
      " [0.10203096]]\n",
      "W1 = [[-0.56333465]]\n",
      "b1 = [[0.1931927]]\n",
      "W2 = [[-0.32262671]\n",
      " [-0.86835542]\n",
      " [-0.87188794]]\n",
      "b2 = [[-0.56789564]\n",
      " [-1.42501386]\n",
      " [-1.42203993]]\n",
      "Cost after iteration 21: 0.002883671467065393\n",
      "y =  [[0.29986759]\n",
      " [0.10167713]\n",
      " [0.10166871]]\n",
      "W1 = [[-0.56432079]]\n",
      "b1 = [[0.19352141]]\n",
      "W2 = [[-0.32251178]\n",
      " [-0.86981113]\n",
      " [-0.87333634]]\n",
      "b2 = [[-0.56776322]\n",
      " [-1.42669099]\n",
      " [-1.42370864]]\n",
      "Cost after iteration 22: 0.0023695739285306823\n",
      "y =  [[0.29989086]\n",
      " [0.10137903]\n",
      " [0.10137186]]\n",
      "W1 = [[-0.56513096]]\n",
      "b1 = [[0.19379147]]\n",
      "W2 = [[-0.322417  ]\n",
      " [-0.87100862]\n",
      " [-0.87452759]]\n",
      "b2 = [[-0.56765408]\n",
      " [-1.42807002]\n",
      " [-1.4250805 ]]\n",
      "Cost after iteration 23: 0.0019482371055243681\n",
      "y =  [[0.29991017]\n",
      " [0.10113446]\n",
      " [0.10112834]]\n",
      "W1 = [[-0.56579698]]\n",
      "b1 = [[0.19401348]]\n",
      "W2 = [[-0.32233897]\n",
      " [-0.87199408]\n",
      " [-0.87550774]]\n",
      "b2 = [[-0.56756425]\n",
      " [-1.42920448]\n",
      " [-1.42620884]]\n",
      "Cost after iteration 24: 0.0016025680658930295\n",
      "y =  [[0.29992614]\n",
      " [0.10093363]\n",
      " [0.10092841]]\n",
      "W1 = [[-0.56634478]]\n",
      "b1 = [[0.19419608]]\n",
      "W2 = [[-0.32227479]\n",
      " [-0.87280533]\n",
      " [-0.87631446]]\n",
      "b2 = [[-0.56749039]\n",
      " [-1.43013811]\n",
      " [-1.42713725]]\n",
      "Cost after iteration 25: 0.0013187373062680304\n",
      "y =  [[0.29993929]\n",
      " [0.10076859]\n",
      " [0.10076415]]\n",
      "W1 = [[-0.56679553]]\n",
      "b1 = [[0.19434633]]\n",
      "W2 = [[-0.32222203]\n",
      " [-0.87347333]\n",
      " [-0.8769786 ]]\n",
      "b2 = [[-0.56742968]\n",
      " [-1.4309067 ]\n",
      " [-1.42790141]]\n",
      "Cost after iteration 26: 0.001085519677877824\n",
      "y =  [[0.29995011]\n",
      " [0.1006329 ]\n",
      " [0.10062912]]\n",
      "W1 = [[-0.56716656]]\n",
      "b1 = [[0.19447]]\n",
      "W2 = [[-0.32217866]\n",
      " [-0.87402351]\n",
      " [-0.87752549]]\n",
      "b2 = [[-0.5673798 ]\n",
      " [-1.4315396 ]\n",
      " [-1.42853052]]\n",
      "Cost after iteration 27: 0.0008937796126823726\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 28, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
